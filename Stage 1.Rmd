---
title: "Stage 1"
author: "Vanessa Stechert"
output:
  pdf_document: default
  word_document: default
---


```{r setup, include=FALSE}
rm(list=ls()) # clean up what is in memory
```

```{r} 
#load packages needed to run the code. 
library(dplyr)
library(ggplot2)
library(tokenizers)
library(tidytext)
library(SnowballC)
library(tm)
library(stringi)
library(ggrepel)

```

```{r}
#Coding in R: <- assigns result on the right to variable on the left. Is similar to "=" sign
# reviews_df<-read.csv('hotel-reviews.csv')
reviews_df <- read.csv("~/Downloads/Amazon_Apple_Phone.csv")
reviews_df <- reviews_df[-c(2)]
names(reviews_df)[3] <- "Description"
View(reviews_df)

reviews_df$Description <- as.character(reviews_df$Description)
print("number of reviews")
nrow(reviews_df)
View(reviews_df)
```
```{r}
#display first two lrows of data
head(reviews_df, n=2)
```



```{r}
#cut text into words by splitting on spaces and punctuation
review_words <- reviews_df %>% unnest_tokens(word,Description,to_lower=FALSE) 
print("number of words")
nrow(review_words)

#Count the number of times each word occurs
counts <- review_words %>%count(word, sort=TRUE) # sort = TRUE for sorting in descending order of n. 
# For questions about a function type ?fun in the console. For example ?count
print("number of unique words")
nrow(counts)

```

Let's keep track of these quantities throughout our cleaning process. What share of these data will we eventually use?

```{r}
# select the most and least mentioned words
counts_high <- head(counts, n = 25) #select the top 25 rows
counts_low <- tail(counts, n = 25) #select the bottom 25 rows
```


```{r}
counts_high %>% 
  mutate(word = reorder(word,n)) %>% 
  top_n(20, word) %>%
  ggplot(aes(word,n)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram") # reorder() function is needed to convert character string to factor in R, such that the plot is made on the number of occurences and not on alphabethic order (which is done when the input is a character vector)

```
Most of the frequent words are words with no meaning. Also mention that the words 'the' and 'The' are both in the list of most frequent words.

Let's clean the data a bit by getting rid of all the stop words.
R has a built in data set with all stop words from different lexicons.
Press ?stop_words in the R console to get more information about this data set.

```{r}
data(stop_words)
review_words_nostop <- review_words %>% 
                        anti_join(stop_words)
counts <- review_words_nostop %>%
            count(word, sort=TRUE)

print("number of words without stop words")
sum(counts$n)
print("number of unique words")
nrow(counts)

```


```{r}
counts %>% 
  mutate(word = reorder(word,n)) %>% 
  top_n(20, word) %>%
  ggplot(aes(word,n)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram")
```
When we filtered out all the stop words, we clearly see that the data are reviews of hotel stays. 


```{r}
#cut text into words by splitting on spaces and punctuation
review_words <- reviews_df %>% unnest_tokens(word,Description,to_lower=TRUE) 
print("number of words")
nrow(review_words)

#Count the number of times each word occurs
counts <- review_words %>%count(word, sort=TRUE) # sort = TRUE for sorting in descending order of n. 
# For questions about a function type ?fun in the console. For example ?count
print("number of unique words")
nrow(counts)

review_words_nostop <- review_words %>% 
                        anti_join(stop_words)
counts <- review_words_nostop %>%
            count(word, sort=TRUE)

print("number of words without stop words")
sum(counts$n)

```

```{r}
counts %>% 
  mutate(word = reorder(word,n)) %>% 
  top_n(20, word) %>%
  ggplot(aes(word,n)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram")
```

These words are much more meaningful
However there is still much overlap in menaing


Temporarily switch back to SLIDES here


```{r, echo=FALSE, message=FALSE}
# Creating the full review from the cleaned+stemmedwords
j<-1
for (j in 1:nrow(reviews_df)) {
 stemmed_description<-  anti_join((reviews_df[j,] %>% unnest_tokens(word,Description, drop=FALSE,to_lower=TRUE) ),stop_words)
  
 stemmed_description<-(wordStem(stemmed_description[,"word"], language = "porter"))

 reviews_df[j,"Description"]<-paste((stemmed_description),collapse = " ")
  
}
```


```{r}
#cut text into words by splitting on spaces and punctuation
review_words <- reviews_df %>% unnest_tokens(word,Description,to_lower=TRUE) 
print("number of words")
nrow(review_words)

#Count the number of times each word occurs
counts <- review_words %>%count(word, sort=TRUE) # sort = TRUE for sorting in descending order of n. 
print("number of unique words after stemming and without stop words")
nrow(counts)
```

What are the most frequent words?
```{r}
counts %>% 
  mutate(word = reorder(word,n)) %>% 
  top_n(20, word) %>%
  ggplot(aes(word,n)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram")
```




Let's first have a look at the most infrequent words.
What are the most infrequent words?


```{r}
counts %>% 
  mutate(word = reorder(word,n)) %>% 
  top_n(-20, word) %>%
  ggplot(aes(word,n)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("Word Frequency Histogram")
```

```{r}
#counting the counts
#Count the number of times each count occurs
counted_counts <- counts  %>%count(n, sort=TRUE) # sort = TRUE for sorting in descending order of n. 


counted_counts %>% 
  mutate(n = reorder(n,nn)) %>% 
    top_n(10, n) %>%
  ggplot(aes(n,nn)) +  
  geom_col() + 
  labs(x = NULL, y = "Number of occurences") + 
  coord_flip() + 
  theme(text = element_text(size = 17)) + 
  ggtitle("count Frequency Histogram")

head(counted_counts,n=11)
```


```{r}
infrequent <- counts %>% filter(n<0.01*nrow(reviews_df))
frequent <- counts[1:2,]
toremove  <- full_join(frequent,infrequent)
print("Number of infrequent words")
nrow(toremove)
print("Share of infrequent words")
nrow(toremove)/nrow(counts)

```


```{r, echo=FALSE, message=FALSE}
j<-1 
for (j in 1:nrow(reviews_df)) {
 stemmed_description <-  anti_join((reviews_df[j,] %>% unnest_tokens(word,Description,to_lower=TRUE)),toremove)
  
 reviews_df[j,"Description"] <- paste((stemmed_description[,"word"]),collapse = " ")

}

save(data,file="amazon_apple_cleaned_reviews.Rdata") 
head(data)
```

```{r}

counts_happy <- review_words_nostop %>%
  filter(review_words_nostop[,2] == c("5","4")) %>% 
           count(word, sort=TRUE)

counts_happy_high <- head(counts_happy, n = 25)

counts_happy_high %>% 
  mutate(word = reorder(word,n)) %>% 
  ggplot(aes(word,n)) +  
  geom_col() + 
  coord_flip() 
```
It looks like that the most frequent words in happy reviews are almost the same as the most frequent words overall. The x-axis is interesting because we see that the frequency gets lower.

```{r}
counts_unhappy <- review_words_nostop %>% 
                    filter(review_words_nostop[,2]==c("3","2","1")) %>%
                    count(word, sort=TRUE)
                    
counts_unhappy_high <- head(counts_unhappy, n = 25)

counts_unhappy_high %>% 
  mutate(word = reorder(word,n)) %>% 
  ggplot(aes(word,n)) +
  geom_col() + 
  coord_flip() 
```

Next week more insightful visualisations will be provided.



Using the tm package to create a cleaned document term matrix faster.

```{r}
#Create corpus
options(stringsAsFactors = FALSE) 
Sys.setlocale('LC_ALL','C')
#Add ID to dataframe to keep track of reviews
reviews <- data.frame(doc_id=seq(1:nrow(reviews_df)),text=reviews_df$Description)
```

```{r}
# Return NA instead of tolower error
tryTolower <- function(x){
# return NA when there is an error
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error = function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}

# Function to clean corpus
clean.corpus<-function(corpus){
  corpus <- tm_map(corpus, content_transformer(tryTolower))
  corpus <- tm_map(corpus, removeWords, custom.stopwords)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
return(corpus)}
```

# Function to stem corpus - For more information: https://cran.r-project.org/web/packages/corpus/vignettes/stemmer.html

data_frame(txt = prideprejudice)
  unnest_tokens(word, txt) %>%
  mutate(word = wordStem(word))
  
```{r}
# List of stopwords to remove from corpus, note that I added hotel and stay as stopwords
custom.stopwords <- c(stopwords('english'), 'iphone','phone','apple')
#meta.data.reader <- DataframeSource(mapping=list(content='text', id='doc_id'))
# pre-cleaned corpus
corpus <- VCorpus(DataframeSource(reviews))
# Clean corpus
corpus_new<-clean.corpus(corpus)

# Stem corpus
corpus_stemmed <- tm_map(corpus_new, stemDocument)

# See how many characters are still inside the corpus for the first row (248 vs 135 vs 122)
# Pre-cleaned & pre-stemmed corpus
inspect(corpus[[1]])
# Cleaned & pre-stemmed corpus
inspect(corpus_new[[1]])
# Cleaned & stemmed corpus
inspect(corpus_stemmed[[1]])
```

```{r}
# With all the infrequent words in here, the TDM will be huge in size. Three options:
  # 1. Remove infrequent words as done above
  # 2. Work with sparse representations
  # 3. use subset of reviews
# Create TermDocumentMatrix with first 20K rows 
tdm<-TermDocumentMatrix(corpus_stemmed[1:20000],control=list(weighting=weightTf))
#Set a max. wordcap to filter sparse words, filtered by frequency
max_words_in_TDM <- 10000
tdm <- tdm[names(tail(sort(rowSums(as.matrix(tdm))), max_words_in_TDM)), ]
tdm.reviews.m<-as.matrix(tdm)
# Dimensions: 10000 (words) x 20000 (documents)
dim(tdm.reviews.m)
tdm.reviews.m[1:25,1:25]
```

```{r}
stri_replace_all_regex('123|456|789', '([0-9]).([0-9])', '$2-$1' )

```

